Description: Use unaligned VMOV instructions
Author: Claude Heiland-Allen <claude@mathr.co.uk>
Bug-Debian: https://bugs.debian.org/939559

--- a/src/gcc/config/i386/i386.md
+++ b/src/gcc/config/i386/i386.md
@@ -2055,7 +2055,7 @@
 	  || misaligned_operand (operands[1], XImode))
 	return "vmovdqu32\t{%1, %0|%0, %1}";
       else
-	return "vmovdqa32\t{%1, %0|%0, %1}";
+	return "vmovdqu32\t{%1, %0|%0, %1}";
 
     default:
       gcc_unreachable ();
@@ -2091,11 +2091,11 @@
       else
 	{
 	  if (get_attr_mode (insn) == MODE_V8SF)
-	    return "vmovaps\t{%1, %0|%0, %1}";
+	    return "vmovups\t{%1, %0|%0, %1}";
 	  else if (get_attr_mode (insn) == MODE_XI)
-	    return "vmovdqa32\t{%1, %0|%0, %1}";
+	    return "vmovdqu32\t{%1, %0|%0, %1}";
 	  else
-	    return "vmovdqa\t{%1, %0|%0, %1}";
+	    return "vmovdqu\t{%1, %0|%0, %1}";
 	}
 
     default:
@@ -2153,11 +2153,11 @@
       else
 	{
 	  if (get_attr_mode (insn) == MODE_V4SF)
-	    return "%vmovaps\t{%1, %0|%0, %1}";
+	    return "%vmovups\t{%1, %0|%0, %1}";
 	  else if (get_attr_mode (insn) == MODE_XI)
-	    return "vmovdqa32\t{%1, %0|%0, %1}";
+	    return "vmovdqu32\t{%1, %0|%0, %1}";
 	  else
-	    return "%vmovdqa\t{%1, %0|%0, %1}";
+	    return "%vmovdqu\t{%1, %0|%0, %1}";
 	}
 
     default:
@@ -2263,14 +2263,14 @@
 	  /* Handle AVX512 registers set.  */
 	  if (EXT_REX_SSE_REG_P (operands[0])
 	      || EXT_REX_SSE_REG_P (operands[1]))
-	    return "vmovdqa64\t{%1, %0|%0, %1}";
-	  return "%vmovdqa\t{%1, %0|%0, %1}";
+	    return "vmovdqu64\t{%1, %0|%0, %1}";
+	  return "%vmovdqu\t{%1, %0|%0, %1}";
 
 	case MODE_V2SF:
 	  gcc_assert (!TARGET_AVX);
 	  return "movlps\t{%1, %0|%0, %1}";
 	case MODE_V4SF:
-	  return "%vmovaps\t{%1, %0|%0, %1}";
+	  return "%vmovups\t{%1, %0|%0, %1}";
 
 	default:
 	  gcc_unreachable ();
@@ -2471,12 +2471,12 @@
 	case MODE_SI:
           return "%vmovd\t{%1, %0|%0, %1}";
 	case MODE_TI:
-	  return "%vmovdqa\t{%1, %0|%0, %1}";
+	  return "%vmovdqu\t{%1, %0|%0, %1}";
 	case MODE_XI:
-	  return "vmovdqa32\t{%g1, %g0|%g0, %g1}";
+	  return "vmovdqu32\t{%g1, %g0|%g0, %g1}";
 
 	case MODE_V4SF:
-	  return "%vmovaps\t{%1, %0|%0, %1}";
+	  return "%vmovups\t{%1, %0|%0, %1}";
 
 	case MODE_SF:
 	  gcc_assert (!TARGET_AVX);
@@ -3351,13 +3351,13 @@
       else
 	{
 	  if (get_attr_mode (insn) == MODE_V4SF)
-	    return "%vmovaps\t{%1, %0|%0, %1}";
+	    return "%vmovups\t{%1, %0|%0, %1}";
 	  else if (TARGET_AVX512VL
 		   && (EXT_REX_SSE_REG_P (operands[0])
 		       || EXT_REX_SSE_REG_P (operands[1])))
-	    return "vmovdqa64\t{%1, %0|%0, %1}";
+	    return "vmovdqu64\t{%1, %0|%0, %1}";
 	  else
-	    return "%vmovdqa\t{%1, %0|%0, %1}";
+	    return "%vmovdqu\t{%1, %0|%0, %1}";
 	}
 
     case TYPE_MULTI:
@@ -3519,11 +3519,11 @@
 	  return "%vmovsd\t{%1, %0|%0, %1}";
 
 	case MODE_V4SF:
-	  return "%vmovaps\t{%1, %0|%0, %1}";
+	  return "%vmovups\t{%1, %0|%0, %1}";
 	case MODE_V8DF:
-	  return "vmovapd\t{%g1, %g0|%g0, %g1}";
+	  return "vmovupd\t{%g1, %g0|%g0, %g1}";
 	case MODE_V2DF:
-	  return "%vmovapd\t{%1, %0|%0, %1}";
+	  return "%vmovupd\t{%1, %0|%0, %1}";
 
 	case MODE_V2SF:
 	  gcc_assert (!TARGET_AVX);
@@ -3713,9 +3713,9 @@
 	  return "%vmovss\t{%1, %0|%0, %1}";
 
 	case MODE_V16SF:
-	  return "vmovaps\t{%g1, %g0|%g0, %g1}";
+	  return "vmovups\t{%g1, %g0|%g0, %g1}";
 	case MODE_V4SF:
-	  return "%vmovaps\t{%1, %0|%0, %1}";
+	  return "%vmovups\t{%1, %0|%0, %1}";
 
 	case MODE_SI:
 	  return "%vmovd\t{%1, %0|%0, %1}";
--- a/src/gcc/config/i386/mmx.md
+++ b/src/gcc/config/i386/mmx.md
@@ -124,16 +124,16 @@
 	    return "%vmovd\t{%1, %0|%0, %1}";
 	  return "%vmovq\t{%1, %0|%0, %1}";
 	case MODE_TI:
-	  return "%vmovdqa\t{%1, %0|%0, %1}";
+	  return "%vmovdqu\t{%1, %0|%0, %1}";
 	case MODE_XI:
-	  return "vmovdqa64\t{%g1, %g0|%g0, %g1}";
+	  return "vmovdqu64\t{%g1, %g0|%g0, %g1}";
 
 	case MODE_V2SF:
 	  if (TARGET_AVX && REG_P (operands[0]))
 	    return "vmovlps\t{%1, %0, %0|%0, %0, %1}";
 	  return "%vmovlps\t{%1, %0|%0, %1}";
 	case MODE_V4SF:
-	  return "%vmovaps\t{%1, %0|%0, %1}";
+	  return "%vmovups\t{%1, %0|%0, %1}";
 
 	default:
 	  gcc_unreachable ();
--- a/src/gcc/config/i386/sse.md
+++ b/src/gcc/config/i386/sse.md
@@ -1001,13 +1001,13 @@
 	      {
 	      case MODE_V8SF:
 	      case MODE_V4SF:
-		return "vmovaps\t{%g1, %g0|%g0, %g1}";
+		return "vmovups\t{%g1, %g0|%g0, %g1}";
 	      case MODE_V4DF:
 	      case MODE_V2DF:
-		return "vmovapd\t{%g1, %g0|%g0, %g1}";
+		return "vmovupd\t{%g1, %g0|%g0, %g1}";
 	      case MODE_OI:
 	      case MODE_TI:
-		return "vmovdqa64\t{%g1, %g0|%g0, %g1}";
+		return "vmovdqu64\t{%g1, %g0|%g0, %g1}";
 	      default:
 		gcc_unreachable ();
 	      }
@@ -1022,7 +1022,7 @@
 	      || misaligned_operand (operands[1], <MODE>mode))
 	    return "%vmovups\t{%1, %0|%0, %1}";
 	  else
-	    return "%vmovaps\t{%1, %0|%0, %1}";
+	    return "%vmovups\t{%1, %0|%0, %1}";
 
 	case MODE_V8DF:
 	case MODE_V4DF:
@@ -1031,7 +1031,7 @@
 	      || misaligned_operand (operands[1], <MODE>mode))
 	    return "%vmovupd\t{%1, %0|%0, %1}";
 	  else
-	    return "%vmovapd\t{%1, %0|%0, %1}";
+	    return "%vmovupd\t{%1, %0|%0, %1}";
 
 	case MODE_OI:
 	case MODE_TI:
@@ -1046,8 +1046,8 @@
 		   ? "vmovdqu<ssescalarsize>\t{%1, %0|%0, %1}"
 		   : "%vmovdqu\t{%1, %0|%0, %1}";
 	  else
-	    return TARGET_AVX512VL ? "vmovdqa64\t{%1, %0|%0, %1}"
-				   : "%vmovdqa\t{%1, %0|%0, %1}";
+	    return TARGET_AVX512VL ? "vmovdqu64\t{%1, %0|%0, %1}"
+				   : "%vmovdqu\t{%1, %0|%0, %1}";
 	case MODE_XI:
 	  if (misaligned_operand (operands[0], <MODE>mode)
 	      || misaligned_operand (operands[1], <MODE>mode))
@@ -1057,7 +1057,7 @@
 		   ? "vmovdqu<ssescalarsize>\t{%1, %0|%0, %1}"
 		   : "vmovdqu64\t{%1, %0|%0, %1}";
 	  else
-	    return "vmovdqa64\t{%1, %0|%0, %1}";
+	    return "vmovdqu64\t{%1, %0|%0, %1}";
 
 	default:
 	  gcc_unreachable ();
@@ -1118,7 +1118,7 @@
       if (misaligned_operand (operands[1], <MODE>mode))
 	return "vmovdqu<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
       else
-	return "vmovdqa<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
+	return "vmovdqu<ssescalarsize>\t{%1, %0%{%3%}%N2|%0%{%3%}%N2, %1}";
     }
 }
   [(set_attr "type" "ssemov")
@@ -1183,7 +1183,7 @@
       if (misaligned_operand (operands[0], <MODE>mode))
 	return "vmovdqu<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
       else
-	return "vmovdqa<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
+	return "vmovdqu<ssescalarsize>\t{%1, %0%{%2%}|%0%{%2%}, %1}";
     }
 }
   [(set_attr "type" "ssemov")
@@ -6825,7 +6825,7 @@
   "TARGET_SSE && !(MEM_P (operands[0]) && MEM_P (operands[1]))"
   "@
    %vmovlps\t{%1, %0|%q0, %1}
-   %vmovaps\t{%1, %0|%0, %1}
+   %vmovups\t{%1, %0|%0, %1}
    %vmovlps\t{%1, %d0|%d0, %q1}"
   [(set_attr "type" "ssemov")
    (set_attr "prefix" "maybe_vex")
@@ -18950,27 +18950,27 @@
       switch (get_attr_mode (insn))
 	{
 	case MODE_V16SF:
-	  return "vmovaps\t{%1, %t0|%t0, %1}";
+	  return "vmovups\t{%1, %t0|%t0, %1}";
 	case MODE_V8DF:
-	  return "vmovapd\t{%1, %t0|%t0, %1}";
+	  return "vmovupd\t{%1, %t0|%t0, %1}";
 	case MODE_V8SF:
-	  return "vmovaps\t{%1, %x0|%x0, %1}";
+	  return "vmovups\t{%1, %x0|%x0, %1}";
 	case MODE_V4DF:
-	  return "vmovapd\t{%1, %x0|%x0, %1}";
+	  return "vmovupd\t{%1, %x0|%x0, %1}";
 	case MODE_XI:
 	  if (which_alternative == 2)
-	    return "vmovdqa\t{%1, %t0|%t0, %1}";
+	    return "vmovdqu\t{%1, %t0|%t0, %1}";
 	  else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-	    return "vmovdqa64\t{%1, %t0|%t0, %1}";
+	    return "vmovdqu64\t{%1, %t0|%t0, %1}";
 	  else
-	    return "vmovdqa32\t{%1, %t0|%t0, %1}";
+	    return "vmovdqu32\t{%1, %t0|%t0, %1}";
 	case MODE_OI:
 	  if (which_alternative == 2)
-	    return "vmovdqa\t{%1, %x0|%x0, %1}";
+	    return "vmovdqu\t{%1, %x0|%x0, %1}";
 	  else if (GET_MODE_SIZE (<ssescalarmode>mode) == 8)
-	    return "vmovdqa64\t{%1, %x0|%x0, %1}";
+	    return "vmovdqu64\t{%1, %x0|%x0, %1}";
 	  else
-	    return "vmovdqa32\t{%1, %x0|%x0, %1}";
+	    return "vmovdqu32\t{%1, %x0|%x0, %1}";
 	default:
 	  gcc_unreachable ();
 	}
